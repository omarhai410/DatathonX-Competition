{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   tweet  sarcasm sentiment  \\\n0      \"د. #محمود_العلايلي:أرى أن الفريق #أحمد_شفيق ر...    False       NEU   \n1      \"مع فيدرر يا آجا والكبار 😍 https://t.co/hrBeHb...    False       NEU   \n2      “الداعون لمبدأ الاختلاط بين الجنسين؛ كالداعين ...     True       NEG   \n3      \"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...     True       NEG   \n4      \"قل شرق حلب ولا تقل حلب الشرقية ....وقل غرب حل...    False       NEU   \n...                                                  ...      ...       ...   \n15543  تربكني الذكرى ليا مر طاريهوانسى البشر حولي وال...    False       NEU   \n15544                 وانا احسبهم الحين مايتركون حركاتهم    False       NEG   \n15545  #فههههههههههد_غشششششششاااااااام_الببببببصصصصصم...    False       NEU   \n15546  - لــو كــان الامـــر بيدي لأخفيت انهيار دموعي...    False       NEG   \n15547  َ نِسااء بابل كنّ يسْحرن الرّجل بالعيُون فقط ....     True       NEG   \n\n      dialect  \n0         msa  \n1         msa  \n2         msa  \n3        gulf  \n4         msa  \n...       ...  \n15543     msa  \n15544     msa  \n15545     msa  \n15546     msa  \n15547     msa  \n\n[15548 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>sarcasm</th>\n      <th>sentiment</th>\n      <th>dialect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"د. #محمود_العلايلي:أرى أن الفريق #أحمد_شفيق ر...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"مع فيدرر يا آجا والكبار 😍 https://t.co/hrBeHb...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>“الداعون لمبدأ الاختلاط بين الجنسين؛ كالداعين ...</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>gulf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"قل شرق حلب ولا تقل حلب الشرقية ....وقل غرب حل...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15543</th>\n      <td>تربكني الذكرى ليا مر طاريهوانسى البشر حولي وال...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15544</th>\n      <td>وانا احسبهم الحين مايتركون حركاتهم</td>\n      <td>False</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15545</th>\n      <td>#فههههههههههد_غشششششششاااااااام_الببببببصصصصصم...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15546</th>\n      <td>- لــو كــان الامـــر بيدي لأخفيت انهيار دموعي...</td>\n      <td>False</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15547</th>\n      <td>َ نِسااء بابل كنّ يسْحرن الرّجل بالعيُون فقط ....</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n  </tbody>\n</table>\n<p>15548 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"training_data.csv\")\n",
    "test = pd.read_csv(\"testing_data.csv\")\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:20.821873600Z",
     "start_time": "2024-02-18T23:14:19.901469Z"
    }
   },
   "id": "9ee0bbb0626ce380"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet\n",
      "\"RT @M__albugaily: قردوغان حليف اسرائيل يدعم تنظيم داعش في سوريا ومع ذلك لم يغضب عليه أحد ويهدده كما يفعل مع مصر التي تواجه الإرهاب بشكل عام…\"    3\n",
      "\"RT @ahlalsunna2: #سوريا :الجيش السوري الحر يسيطر على قرى براغيدة والشيخ ريح والبل وجارز بريف حلب الشمالي بعد معارك مع تنظيم داعش\"                3\n",
      "\"RT @ajmubasher: وزير خارجية #بريطانيا يدعو للتظاهر ضد #روسيا#حلب #سورياhttps://t.co/0eBxBmcxua\"                                                  2\n",
      "\"RT @al8nas_ksa: نائب العام لجماعة #الإخوان_المسلمين في #سوريا ..#إيران تريد تسليم سوريا للإخوان كونهم الأقرب لهم . #يا_إخوان #أردوغان h…\"        2\n",
      "الصحافة أحاديث مع أناس يحسنون الكلام ننشرها علي أناس لايحسنون القراءة. أنيس منصور                                                                 2\n",
      "                                                                                                                                                 ..\n",
      "كان الأولى أن يكتبوا على جميع حوائط المدينة: حافظوا على نظافة نفسيتكم                                                                             1\n",
      "إنت فاكر إن متابعتك لأفلامي أهم من بلدي؟ في ستين ألف داهية وإوعى ترجع في كلامك قبل ما تعتذر على جهلك                                              1\n",
      "\"بالاسعار.. إحصلي على #إطلالة #إليسا المميّزةelissakh https://t.co/cTbfN3AmVc https://t.co/6LNNABTX0r\"                                            1\n",
      "\"الفريق #احمد_شفيق تم رفع اسمه من قوائم الترقب و المنع من السفر https://t.co/OdB0bGfjYQ\"                                                          1\n",
      "َ نِسااء بابل كنّ يسْحرن الرّجل بالعيُون فقط .!! وَ نِسااءْ اليَوم يسْحَرن الرّجل بجَسدْ كاامل .!!                                               1\n",
      "Name: count, Length: 15481, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "data = pd.DataFrame(data, columns=[\"tweet\", \"sentiment\"])\n",
    "print(data[\"tweet\"].value_counts()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:20.835182600Z",
     "start_time": "2024-02-18T23:14:20.553349700Z"
    }
   },
   "id": "583241c5c4e87882"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEG\n",
      "اف هذا🤢\n"
     ]
    },
    {
     "data": {
      "text/plain": "'فيه ناس بتبعتلي رسائل عتاب رقيقة جداً والله يا جماعة أنا ما عنديش أي نية أكون كاهن ولا ملاك وبقول للخروف يا خروف في وشه ومش هتغير'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['sentiment'][12]) # la classe qui est dans la ligne 12\n",
    "print(data['tweet'][12]) # le tweet qui est dans la ligne 12\n",
    "data[\"tweet\"][225] # le tweet qui est dans la ligne 225"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:21.222881100Z",
     "start_time": "2024-02-18T23:14:20.733651800Z"
    }
   },
   "id": "607638499e0e7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'فيه ناس بتبعتلي رسايل عتاب رقيقة جدا واله يا جماعة انا ما عنديش اي نية اكون كاهن ولا ملاك وبقول لخروف يا خروف في وشه ومش هتغير'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from aaransia import transliterate, SourceLanguageError\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "for i in range(len(data)):\n",
    "    row = data.iloc[i][\"tweet\"]\n",
    "    try:\n",
    "        row = transliterate(row, source='ar', target='ar', universal=True)\n",
    "        data.loc[:, \"tweet\"][i] = row\n",
    "    except SourceLanguageError as e:\n",
    "        print(f\"Erreur de translittération à l'index {i}: {e}\")\n",
    "\n",
    "# Affichage d'un exemple aprés le premier prétraitement\n",
    "data[\"tweet\"][225]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:16:59.696533400Z",
     "start_time": "2024-02-18T23:14:20.908000900Z"
    }
   },
   "id": "24ee327b40df9e89"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اف هذا🤢\n"
     ]
    }
   ],
   "source": [
    " import re\n",
    "\n",
    "# Normalisation des données\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "# Applique la normalisation aux tweets\n",
    "data['tweet'] = data['tweet'].apply(normalize_arabic)\n",
    "\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:05.355872500Z",
     "start_time": "2024-02-18T23:16:59.272183Z"
    }
   },
   "id": "8c2997a5434ed3af"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples avant faire le nettoyage :\n",
      "\"مع فيدر يا اجا والكبار 😍 htps:/t.co/hrBeHbkBNu\"\n",
      "“الداعون لمبدا الاختلاط بين الجنسين؛ كالداعين لالغاء التسعيره كلاهما يريد تصفيه السوق السوداء بجعلها حره.” #الاختلاط\n",
      "\"@ihe_94 @ya78m @amoo5 @badiajnikhar @Oukasafa @reoshalm @Mnory202 مساكين من الصبح و هوما رايحين راجعين عاا غوغل تعبت بدالهم ههه ي\"\n",
      "اف هذا🤢\n",
      "\n",
      "Exemples après faire le nettoyage :\n",
      "مع فيدر يا اجا والكبار \n",
      "الداعون لمبدا الاختلاط بين الجنسين كالداعين لالغاء التسعيره كلاهما يريد تصفيه السوق السوداء بجعلها حره الاختلاط\n",
      " مساكين من الصبح و هوما رايحين راجعين عاا غوغل تعبت بدالهم ه ي\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(\"Exemples avant faire le nettoyage :\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])\n",
    "print(data['tweet'][12])\n",
    "\n",
    "# Applique les modifications\n",
    "\n",
    "# Supprimer les hyperliens\n",
    "data['tweet'] = [re.sub(r'http\\S+ | htps\\S+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les URL\n",
    "data['tweet'] = [re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les mots commençant par @\n",
    "data['tweet'] = [re.sub(r'@\\S+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer # et _\n",
    "data['tweet'] = data['tweet'].str.replace(\"_\", \" \").str.replace(\"#\", \"\")\n",
    "\n",
    "data['tweet'] = data['tweet'].str.replace('. | , | ، | ؛', ' ')\n",
    "\n",
    "# Supprimer les mots réservés sur Twitter\n",
    "data['tweet'] = [re.sub(r'\\bRT\\b | \\bRetweeted\\b', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "data['tweet'] = [re.sub(r'[\\u0660-\\u0669\\u06F0-\\u06F9]+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les caractères consécutifs en double\n",
    "data['tweet'] = [re.sub(r\"(.)\\1{2,}\", r\"\\1\", str(s)) for s in data['tweet']] if len(data['tweet']) > 2 else data['tweet'][:2]\n",
    "\n",
    "data['tweet'] = data['tweet'].str.replace('\\d+', ' ')\n",
    "data['tweet'] = data['tweet'].str.replace('\\n', ' ')\n",
    "data['tweet'] = data['tweet'].str.replace('/', ' ')\n",
    "data['tweet'] = [re.sub(r'[^\\w\\s]', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Remplacer les valeurs nulles par une chaîne de caractères vide\n",
    "data['tweet'] = data['tweet'].fillna('')\n",
    "\n",
    "# Vérifie un exemple\n",
    "print(\"\\nExemples après faire le nettoyage :\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:05.394245900Z",
     "start_time": "2024-02-18T23:17:01.220660100Z"
    }
   },
   "id": "5d6c8e0a41d1676a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n",
      "\n",
      "Exemples aprés faire le nettoyage des stop-words:\n",
      "فيدر اجا والكبار\n",
      "الداعون لمبدا الاختلاط الجنسين كالداعين لالغاء التسعيره يريد تصفيه السوق السوداء بجعلها حره الاختلاط\n",
      "مساكين الصبح هوما رايحين راجعين عاا غوغل تعبت بدالهم\n",
      "اف\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Affiche quelques mots (stop-words)\n",
    "print(stopwords.words('arabic'))\n",
    "\n",
    "stopwords = list(set(nltk.corpus.stopwords.words('arabic')))\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "# Affichage un exemple apés faire le nettoyage des données (stop-words)\n",
    "print(\"\\nExemples aprés faire le nettoyage des stop-words:\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:27.037772300Z",
     "start_time": "2024-02-18T23:17:05.260971100Z"
    }
   },
   "id": "b42a916450cb221d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اف\n"
     ]
    }
   ],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "# Stemmer_LIGHT : Supprimer \"suffixes\" et \"affixes\" \n",
    "ArListem = ArabicLightStemmer()\n",
    "def stemmer_light(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "# Root Stemming : Transformer le mot dans sa forme racine\n",
    "def stemmer_root(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "sentences = [stemmer_light(text) for text in data['tweet']]\n",
    "\n",
    "data['tweet'] = sentences\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:19.381450Z",
     "start_time": "2024-02-18T23:17:27.015716100Z"
    }
   },
   "id": "202978e5e4add8e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('عل', 2710),\n",
      " ('ان', 2470),\n",
      " ('لا', 1311),\n",
      " ('له', 1291),\n",
      " ('مصر', 1098),\n",
      " ('tco', 1067),\n",
      " ('لي', 1032),\n",
      " ('نا', 853),\n",
      " ('نت', 799),\n",
      " ('على', 757),\n",
      " ('حد', 644),\n",
      " ('حلب', 599),\n",
      " ('فا', 577),\n",
      " ('كل', 553),\n",
      " ('قول', 547),\n",
      " ('مش', 541),\n",
      " ('رييس', 527),\n",
      " ('دول', 526),\n",
      " ('او', 483),\n",
      " ('نه', 469)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in  vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Nous inspectons le mot principal dans un échantillon de notre corpus sans dériver les mots\n",
    "from pprint import pprint\n",
    "\n",
    "topWords=get_top_n_words(data['tweet'],n=20)\n",
    "pprint(topWords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.235798500Z",
     "start_time": "2024-02-18T23:19:19.034426800Z"
    }
   },
   "id": "c283a8e07bc214cc"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arabic_reshaper'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01marabic_reshaper\u001B[39;00m\n\u001B[0;32m      4\u001B[0m arab_stopwords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(nltk\u001B[38;5;241m.\u001B[39mcorpus\u001B[38;5;241m.\u001B[39mstopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marabic\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[0;32m      5\u001B[0m sample_corpus\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'arabic_reshaper'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import arabic_reshaper\n",
    "\n",
    "arab_stopwords = list(set(nltk.corpus.stopwords.words(\"arabic\")))\n",
    "sample_corpus=' '.join(data['tweet'])\n",
    "data_arb = arabic_reshaper.reshape(sample_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:37:49.773997700Z",
     "start_time": "2024-02-18T23:37:49.695534Z"
    }
   },
   "id": "e6c08a1dd619d489"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_datasets= []\n",
    "\n",
    "print(data[\"sentiment\"].value_counts())\n",
    "sentiment = list(data[\"sentiment\"].unique())\n",
    "print(sentiment)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"tweet\"])\n",
    "vocab_size = len(vectorizer.vocabulary_)\n",
    "print(f\"Il y a {vocab_size} mots différents dans notre corpus.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.510610400Z"
    }
   },
   "id": "82c21ac355f91e02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenizedWords = []\n",
    "documents = []\n",
    "\n",
    "# Chaque document contient tuple ==> la liste de mot et categorie\n",
    "for i in data.index:\n",
    "    sentiment = data[\"sentiment\"][i]\n",
    "    review = data[\"tweet\"][i]\n",
    "    tokenizedWord = word_tokenize(review)\n",
    "    document = [tokenizedWord, sentiment]\n",
    "    documents.append(document)\n",
    "\n",
    "# Chaque element de documents contient une liste = [contient tous les mots de document, son categorie]\n",
    "sentiment\n",
    "print(\"Taille documents : \", len(documents))\n",
    "data[\"tweet\"][4]\n",
    "# Chaque élément de documents contient une liste qui contient tous les mots présents dans ce document et son catégorie (sentiment soit 0 ou bien 1) comme par exemple ce commentaire \n",
    "documents[4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.520108300Z"
    }
   },
   "id": "5dbeb87e8aad7697"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data['tweet'][12])\n",
    "\n",
    "listeall=[]\n",
    "\n",
    "for i in data[\"tweet\"]:\n",
    "    review = i\n",
    "    tokenizedWord = word_tokenize(review)\n",
    "    for j in tokenizedWord:\n",
    "        listeall.append(j)\n",
    "# listeall est une liste qui contient tous les mots de tous les documents , qu'on va utiliser par la suite dans la partie de bag of word \n",
    "print(len(listeall))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.529228800Z"
    }
   },
   "id": "bb4fe5f8418da2d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in listeall)\n",
    "word_features = list(all_words)[:2000]\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "df_fdist = pd.DataFrame.from_dict(all_words, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "print(df_fdist)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.794200100Z",
     "start_time": "2024-02-18T23:19:25.559637100Z"
    }
   },
   "id": "971d3a7bd7518976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# Former le classificateur Naive Bayes\n",
    "model = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Tester le modèle\n",
    "print(\"Accuracy avec le model Naive Bayes : \", nltk.classify.accuracy(model, test_set), \"%\")\n",
    "test_set[0][0]\n",
    "model.classify(test_set[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.576891200Z"
    }
   },
   "id": "d58a0dd5ce38c696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_result.append(model.classify(test_set[i][0]))\n",
    "    gold_result.append(test_set[i][1])\n",
    "\n",
    "CM = nltk.ConfusionMatrix(gold_result, test_result)\n",
    "print(CM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.599693900Z"
    }
   },
   "id": "4be43b2ab74f44f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.show_most_informative_features(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.638610100Z"
    }
   },
   "id": "8d0b64bd3e36232c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des données textuelles\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# Séparation des données en 2 parties : entraînement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Entraînement du modèle\n",
    "modelMulti = MultinomialNB()\n",
    "modelMulti.fit(train_X, train_y)\n",
    "\n",
    "# Test du classificateur\n",
    "predictions = modelMulti.predict(test_X)\n",
    "\n",
    "# Calcul des métriques de performance\n",
    "score_NVB = accuracy_score(test_y, predictions) * 100\n",
    "precision_NVB = precision_score(test_y, predictions, average = None) * 100\n",
    "recall_NVB = recall_score(test_y, predictions, average = None) * 100\n",
    "\n",
    "# Affichage des résultats\n",
    "print('Accuracy du modèle sur l\\'ensemble de test : ', round(score_NVB, 2), '%')\n",
    "report = classification_report(test_y, predictions)\n",
    "print(report) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.684793500Z"
    }
   },
   "id": "d0d6397b40cb14af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for N in range(1,10):\n",
    "    vectorizer_G = CountVectorizer(max_features=8000, ngram_range=(1,N))\n",
    "\n",
    "    X = data[\"tweet\"]\n",
    "    y = data[\"sentiment\"]\n",
    "\n",
    "    # Séparation des données en 2 parties : entraînement / test\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "    train_X = vectorizer_G.fit_transform(train_X)\n",
    "    test_X = vectorizer_G.transform(test_X)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    modelMulti_G = MultinomialNB()\n",
    "    modelMulti_G.fit(train_X, train_y)\n",
    "    \n",
    "    # Test du classificateur\n",
    "    predictions_2G = modelMulti_G.predict(test_X)\n",
    "    print('Accuracy du modèle avec {} gramme: '.format(N), round(accuracy_score(test_y, predictions_2G) * 100, 2), '%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.695306600Z"
    }
   },
   "id": "2500bae76258f607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "vectorizer_G = CountVectorizer(max_features=8000, ngram_range=(1,3))\n",
    "\n",
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# Séparation des données en 2 parties : entraînement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer_G.fit_transform(train_X) \n",
    "test_X = vectorizer_G.transform(test_X)\n",
    "\n",
    "# Entraînement du modèle\n",
    "modelMulti_G = MultinomialNB()\n",
    "modelMulti_G.fit(train_X, train_y)\n",
    "\n",
    "# Test the classifier\n",
    "predictions_G = modelMulti_G.predict(test_X)\n",
    "score_NVB_G = accuracy_score(test_y, predictions_G) * 100\n",
    "precision_NVB_G = precision_score(test_y, predictions_G, average = None) * 100\n",
    "recall_NVB_G = recall_score(test_y, predictions_G, average = None) * 100\n",
    "print('Accuracy du modèle sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_G) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions_G)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.729164600Z"
    }
   },
   "id": "ef8772629236bdda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features=8000)\n",
    "\n",
    "\n",
    "# Séparation des données en 2 parties : entraînement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Entraînement du modèle\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=20, min_samples_split=2, min_samples_leaf=1)\n",
    "decision_tree = decision_tree.fit(train_X, train_y)\n",
    "predictions_tree = decision_tree.predict(test_X)\n",
    "score_DT = accuracy_score(test_y, predictions_tree) * 100\n",
    "precision_DT = precision_score(test_y, predictions_tree, average = None) * 100\n",
    "recall_DT = recall_score(test_y, predictions_tree, average = None) * 100\n",
    "print('Accuracy du modèle decision_tree avec CountVectorizer sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_tree) * 100, 2), '%')\n",
    "\n",
    "report = classification_report(test_y, predictions_tree)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.777193100Z"
    }
   },
   "id": "497e8fb044699be4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "\n",
    "\n",
    "# Séparation des données en 2 parties : entraînement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Entraînement du modèle\n",
    "rf = RandomForestClassifier(max_depth = None, random_state = 32, n_estimators = 1000)\n",
    "rf = rf.fit(train_X, train_y)\n",
    "predictions = rf.predict(test_X)\n",
    "score_RF = accuracy_score(test_y, predictions) * 100\n",
    "precision_RF = precision_score(test_y, predictions, average = None) * 100\n",
    "recall_RF = recall_score(test_y, predictions, average = None) * 100\n",
    "print('Accuracy du modèle Random_Forest avec CountVectorizer sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions)\n",
    "print(report)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.893865Z",
     "start_time": "2024-02-18T23:19:25.799488400Z"
    }
   },
   "id": "5dc45f6ae86dc9ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "vectorizer_TFIDF = TfidfVectorizer(max_features=8000)\n",
    "\n",
    "# Séparation des données en 2 parties : entraînement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer_TFIDF.fit_transform(train_X) \n",
    "test_X = vectorizer_TFIDF.transform(test_X)\n",
    "\n",
    "# Entraînement du modèle\n",
    "modelMulti_tf = MultinomialNB()\n",
    "modelMulti_tf.fit(train_X, train_y)\n",
    "\n",
    "# Test the classifier\n",
    "predictions_TF = modelMulti_tf.predict(test_X)\n",
    "score_NVB_TF = accuracy_score(test_y, predictions_TF) * 100\n",
    "precision_NVB_TF = precision_score(test_y, predictions_TF, average = None) * 100\n",
    "recall_NVB_TF= recall_score(test_y, predictions_TF, average = None) * 100\n",
    "print('Accuracy du modèle MultinomialNB avec TfidfVectorizer et sans n-grames sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_TF) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions_TF)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.829837800Z"
    }
   },
   "id": "b3496a7947f2ae19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modèle 01\n",
    "data.columns = [\"tweet\", \"sentiment\"]\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features = 8000, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(data[\"tweet\"])\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_cl = lin_clf.fit(train_X, train_y)\n",
    "predictions = lin_cl.predict(val_X)\n",
    "score_svm = accuracy_score(val_y, predictions) * 100\n",
    "precision_svm = precision_score(val_y, predictions, average=None) * 100\n",
    "recall_svm = recall_score(val_y, predictions, average=None) * 100\n",
    "\n",
    "print('Accuracy du modèle SVM avec CountVectorizer et n-grames sur l\\'ensemble de test : ', round(score_svm, 2), '%')\n",
    "report = classification_report(val_y, predictions)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.873534900Z"
    }
   },
   "id": "4cbcb8d25d3a2fd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = np.array([\n",
    "    [score_NVB, precision_NVB[0], recall_NVB[0], precision_NVB[1], recall_NVB[1], precision_NVB[2], recall_NVB[2]],\n",
    "    [score_DT, precision_DT[0], recall_DT[0], precision_DT[1], recall_DT[1], precision_DT[2], recall_DT[2]],\n",
    "    [score_RF, precision_RF[0], recall_RF[0], precision_RF[1], recall_RF[1], precision_RF[2], recall_RF[2]],\n",
    "    [score_svm, precision_svm[0], recall_svm[0], precision_svm[1], recall_svm[1], precision_svm[2], recall_svm[2]]\n",
    "])\n",
    "\n",
    "# Créer un DataFrame pandas avec les colonnes appropriées\n",
    "columns = ['accuracy', 'precision_NEG', 'recall_NEG', 'precision_NEU', 'recall_NEU', 'precision_POS', 'recall_POS']\n",
    "index = ['Naive Bayes', 'Decision Tree', 'Random Forest', 'SVM']\n",
    "\n",
    "df = pd.DataFrame(df_data, columns=columns, index=index)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.887218100Z"
    }
   },
   "id": "268b187442b4a7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = np.array([\n",
    "    [score_NVB, precision_NVB[0], recall_NVB[0], precision_NVB[1], recall_NVB[1], precision_NVB[2], recall_NVB[2]],\n",
    "    [score_NVB_G, precision_NVB_G[0], recall_NVB_G[0], precision_NVB_G[1], recall_NVB_G[1], precision_NVB_G[2], recall_NVB_G[2]],\n",
    "    [score_NVB_TF, precision_NVB_TF[0], recall_NVB_TF[0], precision_NVB_TF[1], recall_NVB_TF[1], precision_NVB_TF[2], recall_NVB_TF[2]],\n",
    "    [score_NVB_TF_G, precision_NVB_TF_G[0], recall_NVB_TF_G[0], precision_NVB_TF_G[1], recall_NVB_TF_G[1], precision_NVB_TF_G[2], recall_NVB_TF_G[2]]\n",
    "])\n",
    "\n",
    "# Créer un DataFrame pandas avec les colonnes appropriées\n",
    "columns = ['accuracy', 'precision_NEG', 'recall_NEG', 'precision_NEU', 'recall_NEU', 'precision_POS', 'recall_POS']\n",
    "index = ['Naive Bayes avec CountVectorizer 1_gram', 'Naive Bayes avec CountVectorizer 3_gram ',\n",
    "         'Naive Bayes avec TfidfVectorizer 1_gram', 'Naive Bayes avec TfidfVectorizer 3_gram']\n",
    "\n",
    "df = pd.DataFrame(df_data, columns=columns, index=index)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.905453300Z",
     "start_time": "2024-02-18T23:19:25.900383900Z"
    }
   },
   "id": "b8be824d8ded1635"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
